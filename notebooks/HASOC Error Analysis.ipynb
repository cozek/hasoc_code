{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "sns.set(style=\"darkgrid\")\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args= argparse.Namespace(\n",
    "    loc = '../data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalsePostive_english_task_1_run_1.tsv FalsePostive_german_task_2_run_3.tsv\r\n",
      "FalsePostive_english_task_2_run_1.tsv FalsePostive_hindi_task_1_run_2.tsv\r\n",
      "FalsePostive_english_task_2_run_2.tsv FalsePostive_hindi_task_2_run_1.tsv\r\n",
      "FalsePostive_english_task_3_run_1.tsv FalsePostive_hindi_task_3_run_1.tsv\r\n",
      "FalsePostive_german_task_1_run_2.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_GOLD = pd.read_csv(\n",
    "    os.path.join(args.loc,'gold','english_data.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "HINDI_GOLD = pd.read_csv(\n",
    "    os.path.join(args.loc,'gold','hindi_data.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "GERMAN_GOLD = pd.read_csv(\n",
    "    os.path.join(args.loc,'gold','german_data.tsv'),\n",
    "    sep='\\t'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN_PREDS = {  \n",
    "    'task_1': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_german_task_1_run_2.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "    'task_2': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_german_task_2_run_3.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_PREDS = {  \n",
    "    'task_1': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_english_task_1_run_1.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "    'task_2': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_english_task_2_run_2.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "    'task_3': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_english_task_3_run_1.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HINDI_PREDS = {  \n",
    "    'task_1': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_hindi_task_1_run_2.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "    'task_2': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_hindi_task_2_run_1.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "    'task_3': pd.read_csv(\n",
    "        os.path.join(args.loc,'runs','FalsePostive_hindi_task_3_run_1.tsv',),\n",
    "        sep='\\t'\n",
    "    ) ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis(\n",
    "    gold_df:pd.DataFrame,\n",
    "    pred_df:pd.DataFrame,\n",
    "    subtask:str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gold_df: DataFrame containing gold labels\n",
    "        preds_df: DataFrame containing model preds\n",
    "        subtask: 1,2 or 3\n",
    "    Returns:\n",
    "        analysis_df: DF containing the columns  Text, True labels as 'true'\n",
    "            predicted labels as 'pred'\n",
    "        cross_tab: A pandas crosstab as confusion matrix\n",
    "    \"\"\"\n",
    "    pred_df.rename(\n",
    "        columns={'result':'pred'},\n",
    "        inplace=True,\n",
    "    )\n",
    "    gold_df = gold_df[['text_id',f'task_{subtask}']]\n",
    "        \n",
    "    analysis_df = pd.merge(\n",
    "        pred_df,\n",
    "        gold_df,\n",
    "        on='text_id',\n",
    "    )\n",
    "    \n",
    "    analysis_df.rename(\n",
    "        columns={f'task_{subtask}': 'true'},\n",
    "        inplace=True\n",
    "    )\n",
    "    cross_tab = pd.crosstab(\n",
    "        analysis_df.true , #y_true\n",
    "        analysis_df.pred , #y_pred\n",
    "        rownames=['True'], colnames=['Predicted'], margins=True\n",
    "    )\n",
    "    return analysis_df, cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_samples(\n",
    "    analysis_df:pd.DataFrame,\n",
    "    mistakes:bool=True,\n",
    "    num_samples:int=5\n",
    ")-> None:\n",
    "    \"\"\"Prints the samples for analysis\"\"\"\n",
    "    \n",
    "    with pd.option_context('display.max_colwidth', -1): \n",
    "        if mistakes: #print misclassifications\n",
    "            df = analysis_df[analysis_df.true != analysis_df.pred]\n",
    "            [['Text','true','pred']]\n",
    "        else: #print correct classifications\n",
    "            df = analysis_df[analysis_df.true == analysis_df.pred]\n",
    "            [['Text','true','pred']]\n",
    "            \n",
    "        print(df.sample(num_samples))\n",
    "        \n",
    "        print('\\n',df['Text'].map(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_id</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hasoc_en_207</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_en_568</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hasoc_en_137</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hasoc_en_214</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hasoc_en_869</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1148</td>\n",
       "      <td>hasoc_en1_7212</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>1149</td>\n",
       "      <td>hasoc_en1_3958</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1150</td>\n",
       "      <td>hasoc_en1_4648</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>1151</td>\n",
       "      <td>hasoc_en1_4832</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1152</td>\n",
       "      <td>hasoc_en1_3721</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1153 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         text_id result\n",
       "0              0    hasoc_en_207    HOF\n",
       "1              1    hasoc_en_568    HOF\n",
       "2              2    hasoc_en_137    HOF\n",
       "3              3    hasoc_en_214    HOF\n",
       "4              4    hasoc_en_869    HOF\n",
       "...          ...             ...    ...\n",
       "1148        1148  hasoc_en1_7212    NOT\n",
       "1149        1149  hasoc_en1_3958    NOT\n",
       "1150        1150  hasoc_en1_4648    NOT\n",
       "1151        1151  hasoc_en1_4832    NOT\n",
       "1152        1152  hasoc_en1_3721    NOT\n",
       "\n",
       "[1153 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_PREDS[f'task_{1}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task = 1\n",
    "eng_task_1_analysis_df, eng_task_1_crosstab =  get_analysis(\n",
    "    gold_df = ENGLISH_GOLD,\n",
    "    pred_df = ENGLISH_PREDS[f'task_{task}'],\n",
    "    subtask = task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF     0.4079    0.4688    0.4362       288\n",
      "         NOT     0.8139    0.7734    0.7931       865\n",
      "\n",
      "    accuracy                         0.6973      1153\n",
      "   macro avg     0.6109    0.6211    0.6147      1153\n",
      "weighted avg     0.7125    0.6973    0.7040      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_true = eng_task_1_analysis_df.true,\n",
    "    y_pred = eng_task_1_analysis_df.pred,\n",
    "    digits = 4,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 2\n",
    "eng_task_2_analysis_df, eng_task_2_crosstab =  get_analysis(\n",
    "    gold_df = ENGLISH_GOLD,\n",
    "    pred_df = ENGLISH_PREDS[f'task_{task}'],\n",
    "    subtask = task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HATE     0.1688    0.3145    0.2197       124\n",
      "        NONE     0.8139    0.7734    0.7931       865\n",
      "        OFFN     0.2308    0.0423    0.0714        71\n",
      "        PRFN     0.2989    0.2796    0.2889        93\n",
      "\n",
      "    accuracy                         0.6392      1153\n",
      "   macro avg     0.3781    0.3524    0.3433      1153\n",
      "weighted avg     0.6671    0.6392    0.6463      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_true = eng_task_2_analysis_df.true,\n",
    "    y_pred = eng_task_2_analysis_df.pred,\n",
    "    digits = 4,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 3\n",
    "eng_task_3_analysis_df, eng_task_3_crosstab = get_analysis(\n",
    "    gold_df = ENGLISH_GOLD,\n",
    "    pred_df = ENGLISH_PREDS[f'task_{task}'],\n",
    "    subtask = task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NONE     0.8139    0.7734    0.7931       865\n",
      "         TIN     0.3652    0.4204    0.3909       245\n",
      "         UNT     0.0816    0.0930    0.0870        43\n",
      "\n",
      "    accuracy                         0.6730      1153\n",
      "   macro avg     0.4202    0.4289    0.4237      1153\n",
      "weighted avg     0.6912    0.6730    0.6813      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_true = eng_task_3_analysis_df.true,\n",
    "    y_pred = eng_task_3_analysis_df.pred,\n",
    "    digits = 4,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Error Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
