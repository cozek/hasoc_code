{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Task A - GPT 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "njzRjFbiQYlu"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cozek/hasoc_code/blob/master/notebooks/English/Task%20A%20-%20GPT%202.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nF2qyeioUpoh"
      },
      "source": [
        "# If working in colab, clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw7CyHbUQVsr",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    __import__('google.colab')\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LG5U3wSYZ_yl",
        "colab": {}
      },
      "source": [
        "if IN_COLAB:\n",
        "    os.system('rm -rf /content/hasoc_code/')\n",
        "    os.system('git clone --recurse-submodules https://github.com/cozek/hasoc_code')\n",
        "    sys.path.append('/content/hasoc_code/src')\n",
        "else:\n",
        "    sys.path.append('../../src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iemxHSBz7SBb"
      },
      "source": [
        "### Install tranformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-0C7RFRQSDa",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    __import__(transformers)\n",
        "except:\n",
        "    os.system(\"git clone https://github.com/huggingface/transformers\")\n",
        "    os.system(\"pip install ./transformers\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uZwwEsY_UuZ6"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDW5zDiUaWnI",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aE7jfIRqUTtn",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "import importlib\n",
        "import pprint\n",
        "import nltk\n",
        "import datetime\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r_rDeBsOUXKi",
        "colab": {}
      },
      "source": [
        "import utils.general as general_utils\n",
        "import utils.data as data_utils\n",
        "import utils.transformer.data as transformer_data_utils\n",
        "import utils.transformer.general as transformer_general_utils\n",
        "general_utils.set_seed_everywhere() #set the seed for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3UfRWC0qUb-a",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_ukzh8WS_Y5",
        "colab": {}
      },
      "source": [
        "#from radam import RAdam\n",
        "#from lookahead import optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JvLdf3pJ67m7",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rjsqG7Z4VB3f"
      },
      "source": [
        "# Set up the argspace/important_variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tcFfJCICTeSK",
        "colab": {}
      },
      "source": [
        "args = Namespace(\n",
        "        #use cuda by default\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "        #apex\n",
        "        use_apex = False,\n",
        "\n",
        "        #model storage\n",
        "        store_model = True,\n",
        "\n",
        "        #set batch size and number of epochs\n",
        "        batch_size = 32,\n",
        "        num_epochs = 20,\n",
        "    \n",
        "        #set the learning rate\n",
        "        learning_rate = 0.0001,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oSWgKzv8YzGi",
        "colab": {}
      },
      "source": [
        "if not IN_COLAB:\n",
        "    _prefix = '../../'\n",
        "else : #for colab\n",
        "    _prefix = '/content/hasoc_code/'\n",
        "    #location of the train, dev and test csv\n",
        "args.data_tsv = _prefix + 'data/english_dataset/english_dataset.tsv'\n",
        "args.test_tsv = _prefix + 'data/english_dataset/hasoc2019_en_test-2919.tsv'\n",
        "if args.store_model:\n",
        "     #directory to save our models at\n",
        "    args.directory = _prefix+'/models/' \n",
        "    args.model_name = 'gpt2_eng_a.pt'\n",
        "    if not os.path.exists(args.directory):\n",
        "        os.makedirs(args.directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xi3534jtUket",
        "colab": {}
      },
      "source": [
        "if args.use_apex:\n",
        "    try:\n",
        "        __import__(apex)\n",
        "    except:\n",
        "        print(\"Installing Nvidia-Apex\")\n",
        "        os.system('export CUDA_HOME=/usr/local/cuda-10.1')\n",
        "        os.system('git clone https://github.com/NVIDIA/apex')\n",
        "        os.system('pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex')\n",
        "        from apex import amp, optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFt_ff9uVgfV"
      },
      "source": [
        "# Load the data tsv for Sub-task A\n",
        "Sub-task A focus on Hate speech and Offensive language identification offered for English, German, Hindi. Sub-task A is coarse-grained binary classification in which participating system are required to classify tweets into two class, namely: Hate and Offensive (HOF) and Non- Hate and offensive (NOT).\n",
        "\n",
        "- (NOT) Non Hate-Offensive - This post does not contain any Hate speech, offensive content.\n",
        "- (HOF) Hate and Offensive - This post contains Hate, offensive, and profane content.\n",
        "\n",
        "In our annotation, we label a post as HOF if it contains any form of non-acceptable language such as hate speech, aggression, profanity otherwise NOT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xLNMib0Kawmg"
      },
      "source": [
        "### Load the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vdNdJuwlVfyT",
        "colab": {}
      },
      "source": [
        "hasoc_data_df = pd.read_csv(args.data_tsv, sep='\\t')\n",
        "hasoc_data_df_task_a = hasoc_data_df[['text','task_1']]\n",
        "hasoc_data_df_task_a.columns.values[1] = 'label'\n",
        "hasoc_data_df_task_a = hasoc_data_df_task_a[hasoc_data_df_task_a.label != 'NONE']\n",
        "del hasoc_data_df #free memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hawbE-56aL5u",
        "colab": {}
      },
      "source": [
        "task_a_label_dict = {'NOT' :0, 'HOF': 1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V2CP-06_Zm8v",
        "colab": {}
      },
      "source": [
        "hasoc_data_df_task_a.label = hasoc_data_df_task_a.label.map(task_a_label_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t8KxsUVSbXKw"
      },
      "source": [
        "### Train set Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8LYlFEX9aCmJ",
        "colab": {}
      },
      "source": [
        "hasoc_data_df_task_a.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iP6lRcHfabEh",
        "colab": {}
      },
      "source": [
        "print(hasoc_data_df_task_a.label.value_counts())\n",
        "hasoc_data_df_task_a.label.value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8sSm6p0o3_Cq"
      },
      "source": [
        "### Split dataframe into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKFGEObIpEAv",
        "colab": {}
      },
      "source": [
        "data_df = data_utils.Datasplitter.split_dataframe(hasoc_data_df_task_a, train_frac= 0.9, shuffle=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bs_phT5RqeeN",
        "colab": {}
      },
      "source": [
        "print(data_df.split.value_counts())\n",
        "sum(data_df.label.value_counts()) == \\\n",
        "sum(data_df[data_df.split == 'train'].label.value_counts())\\\n",
        " + sum(data_df[data_df.split == 'val'].label.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "20Gwn6WizDaB",
        "colab": {}
      },
      "source": [
        "hasoc_data_df_task_a.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "347OqySW6nOD"
      },
      "source": [
        "## Create GPT2 Preprocessor\n",
        "Add special tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wkhlygu1zHko",
        "colab": {}
      },
      "source": [
        "class GPT2Preprocessor():\n",
        "    \"\"\"Adds special tokens to the samples\"\"\"\n",
        "    def __init__(self,transformer_tokenizer,sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "        \n",
        "    def add_eos_tokens(self, text):\n",
        "        eos_token = ' ' + self.transformer_tokenizer.eos_token + ' '\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text  = eos_token.join(sentences) + ' ' + self.transformer_tokenizer.eos_token\n",
        "        return eos_added_text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_5PLkvd7fqI",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "except:\n",
        "    nltk.download('punkt')\n",
        "    punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ujSA_8D6x9V",
        "colab": {}
      },
      "source": [
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "    'gpt2'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwUtLafp61ZI",
        "colab": {}
      },
      "source": [
        "gpt2_preproc = GPT2Preprocessor(gpt2_tokenizer, punkt_sentence_detector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LESIUwL27z-Q",
        "colab": {}
      },
      "source": [
        "data_df['text'] = data_df['text'].map(gpt2_preproc.add_eos_tokens)\n",
        "print(data_df.columns)\n",
        "print(data_df.label.value_counts())\n",
        "print(data_df.split.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LuuOLpVO79dT",
        "colab": {}
      },
      "source": [
        "with pd.option_context('display.max_colwidth', -1): \n",
        "    print(data_df[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CHvd35358mkx"
      },
      "source": [
        "## Create the pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L769KdUr8A3v",
        "colab": {}
      },
      "source": [
        "dataset = transformer_data_utils.HateDataset(\n",
        "    data_df = data_df,\n",
        "    tokenizer = gpt2_tokenizer,\n",
        "    max_len = 512,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJKD6KYOpvUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_data_utils.HateDataset?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2amAq-n83O0"
      },
      "source": [
        "### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GLyi18XQ82ZP",
        "colab": {}
      },
      "source": [
        "class SimpleGPT2SequenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        num_classes:int,\n",
        "        max_seq_len:int,\n",
        "        gpt_model_name:str,\n",
        "    ):\n",
        "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
        "        self.gpt2model = GPT2Model.from_pretrained(\n",
        "            gpt_model_name,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x_in):\n",
        "        \n",
        "        gpt_out = self.gpt2model(x_in)[0] #returns tuple\n",
        "        batch_size = gpt_out.shape[0]\n",
        "        prediction_vector = self.fc1(gpt_out.view(batch_size,-1)) #(batch_size , max_len, num_classes)\n",
        "    \n",
        "        return prediction_vector\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qFZZQLwW8q-1",
        "colab": {}
      },
      "source": [
        "num_classes = len(set(data_df.label))\n",
        "hidden_size = min(dataset._max_seq_length,1024) * 768\n",
        "model = SimpleGPT2SequenceClassifier(\n",
        "    hidden_size=hidden_size, \n",
        "    num_classes = num_classes,\n",
        "    gpt_model_name = 'gpt2',\n",
        "    max_seq_len = dataset._max_seq_length,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "De_MpU0ICwBh"
      },
      "source": [
        "## Training Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZENphUjyACq4",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "model = model.to(args.device)\n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "print(f'Using LR:{args.learning_rate}')\n",
        "early_stopping = transformer_general_utils.EarlyStopping(patience=4)\n",
        "optimizer = optim.AdamW(model.parameters(), lr = args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer = optimizer,\n",
        "    mode = 'min',\n",
        "    factor =0.5,\n",
        "    patience = 1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMe9uMdLDAsV",
        "colab": {}
      },
      "source": [
        "train_state = general_utils.make_train_state() #dictionary for saving training routine information\n",
        "train_state.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHUMOkShDBmc",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cc0D1htBDUwa",
        "colab": {}
      },
      "source": [
        "if args.use_apex:\n",
        "    print(\"Using Nvidia-Apex\")\n",
        "    opt_level = 'O1'\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Q2ixmdeDu6G",
        "colab": {}
      },
      "source": [
        "args.batch_size = 4 # depending on system"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NIvv0U0Cu1Z",
        "colab": {}
      },
      "source": [
        "epoch_bar = notebook.tqdm(\n",
        "    desc = 'training_routine',\n",
        "    total = args.num_epochs,\n",
        "    position=0,\n",
        "    leave = True,\n",
        ")\n",
        "dataset.set_split('train')\n",
        "train_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "dataset.set_split('val')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=eval',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_in'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = transformer_data_utils.generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = False, n_workers = 2, \n",
        "    )\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.train()\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(\n",
        "            batch_dict['x_data'],\n",
        "        )\n",
        "        y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "                             \n",
        "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "        if args.use_apex:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            \n",
        "        optimizer.step()\n",
        "                             \n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "                             \n",
        "        y_pred = y_pred.detach().cpu()\n",
        "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "        \n",
        "        acc_t = transformer_general_utils \\\n",
        "            .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "        \n",
        "        f1_t = transformer_general_utils \\\n",
        "            .compute_macro_f1(y_pred, batch_dict['y_target'], average='weighted')\n",
        "\n",
        "        train_state['batch_preds'].append(y_pred)\n",
        "        train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "        train_state['batch_indexes'].append(batch_dict['x_index'])\n",
        "\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                             epoch=epoch_index)\n",
        "\n",
        "        train_bar.update()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    train_state['train_accuracies'].append(running_acc)\n",
        "    train_state['train_losses'].append(running_loss)\n",
        "    \n",
        "    train_state['train_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "    train_state['train_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['train_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    train_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['train_preds'][-1],\n",
        "                                  train_state['train_targets'][-1],\n",
        "                                  'weighted'\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['train_f1s'].append(train_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    \n",
        "    dataset.set_split('val')\n",
        "    batch_generator = transformer_data_utils.generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # optimizer._backup_and_load_cache()\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(\n",
        "                batch_dict['x_data'],\n",
        "            )\n",
        "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "#             batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "            \n",
        "            acc_t = transformer_general_utils\\\n",
        "                .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            f1_t = transformer_general_utils \\\n",
        "                .compute_macro_f1(y_pred, batch_dict['y_target'],\n",
        "                                 average='weighted')\n",
        "\n",
        "            train_state['batch_preds'].append(y_pred.cpu())\n",
        "            train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "            \n",
        "\n",
        "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                                 epoch=epoch_index)\n",
        "            eval_bar.update()\n",
        "            \n",
        "    train_state['val_accuracies'].append(running_acc)\n",
        "    train_state['val_losses'].append(running_loss)\n",
        "    \n",
        "        \n",
        "    train_state['val_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['val_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    val_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['val_preds'][-1],\n",
        "                                  train_state['val_targets'][-1],\n",
        "                                  average='weighted',\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['val_f1s'].append(val_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'model':model.state_dict(),\n",
        "        },\n",
        "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
        "    )\n",
        "    \n",
        "    scheduler.step(val_f1)\n",
        "    early_stopping(val_f1, model)\n",
        "    # optimizer._clear_and_load_backup()\n",
        "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()    \n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1 )\n",
        "    epoch_bar.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3gv3LxlDW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}